{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5T8vrrFSvYOz",
        "oVaS_vHpxjCb"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOXhyAKs7mmZ5bpjP7fjmeZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pris25123/synthetic-data-generation/blob/main/colabfile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Links to dataset and fine tuned model on hugging face\n",
        "1. [dataset](https://huggingface.co/datasets/foreseeitwithme/real-estate-qa-synthetic)\n",
        "\n",
        "2. [finetuned model](https://huggingface.co/foreseeitwithme/real-estate-qa-synthetic)"
      ],
      "metadata": {
        "id": "c8q-uoXUztgN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c44ca30f"
      },
      "source": [
        "## Synthetic Data Generation and LLM Fine-tuning\n",
        "\n",
        "### Overview\n",
        "1. Create a synthetic dataset for a use case of your choice\n",
        "2. Fine-tune a small LLM using this dataset\n",
        "3. Evaluate the model performance before and after fine-tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I wanted to create a smart real estate Q&A system that can help people get quick, accurate answers about property buying, selling, and legal processes. Since real estate data is often limited or scattered, I generated my own synthetic dataset and fine-tuned a powerful language model to understand and respond well to these specific questions. This makes it easier for users—whether buyers, sellers, or agents—to get reliable info fast without digging through complicated documents."
      ],
      "metadata": {
        "id": "TUj226kZzSB2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781ea0f2"
      },
      "source": [
        "## 2. Environment Setup\n",
        "\n",
        "In this section, we'll install all the necessary dependencies for our project. This includes libraries for:\n",
        "- Data processing and manipulation\n",
        "- LLM access and fine-tuning\n",
        "- Evaluation metrics\n",
        "- Hugging Face integration for dataset upload and model download\n",
        "\n",
        "Run the cell below to set up your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33819aa6"
      },
      "outputs": [],
      "source": [
        "# Install necessary dependencies\n",
        "!pip install -q transformers datasets evaluate peft bitsandbytes accelerate\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q trl\n",
        "!pip install -q nltk rouge-score sacrebleu\n",
        "\n",
        "# Optional: For specific use cases\n",
        "# !pip install -q sentencepiece tokenizers\n",
        "# !pip install -q gradio # For demo creation\n",
        "\n",
        "# Login to Hugging Face (you'll need a token)\n",
        "from huggingface_hub import login\n",
        "# Uncomment the line below and add your token when ready to upload datasets\n",
        "# login()\n",
        "\n",
        "# Verify installations\n",
        "import transformers\n",
        "import datasets\n",
        "import peft\n",
        "\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "print(f\"PEFT version: {peft.__version__}\")\n",
        "\n",
        "# Check available GPU\n",
        "!nvidia-smi\n",
        "# ideally a T4 or A100 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff9b07cc"
      },
      "source": [
        "## 3. Synthetic Data Generation\n",
        "\n",
        "In this section, we'll generate a synthetic dataset for our selected use case. The process involves:\n",
        "\n",
        "1. Defining the data structure and schema\n",
        "2. Setting up data generation techniques (LLM prompting, rules-based generation, etc.)\n",
        "3. Creating the dataset\n",
        "4. Validating data quality\n",
        "5. Uploading to Hugging Face Datasets\n",
        "\n",
        "Some libraries you can use for data generation:\n",
        "- https://github.com/meta-llama/synthetic-data-kit\n",
        "- https://github.com/argilla-io/distilabel\n",
        "- https://github.com/argilla-io/synthetic-data-generator\n",
        "\n",
        "For llms you can use local llm , use free apis from [groq](https://groq.com/) anything else you can find."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Question templates\n",
        "questions_templates = [\n",
        "    \"What documents are required for {scenario}?\",\n",
        "    \"How do I verify the authenticity of a {entity}?\",\n",
        "    \"What is the average price of a {property_type} in {location}?\",\n",
        "    \"Are there any legal considerations when buying {property_type} in {location}?\",\n",
        "    \"What is the difference between a {term1} and a {term2}?\"\n",
        "]\n",
        "\n",
        "# Options to fill the templates\n",
        "scenarios = [\n",
        "    \"buying a flat in Bangalore\",\n",
        "    \"registering a new property\",\n",
        "    \"applying for a home loan\",\n",
        "    \"transferring property ownership\",\n",
        "    \"selling a commercial property\"\n",
        "]\n",
        "\n",
        "entities = [\"property\", \"title deed\", \"builder\", \"property tax receipt\"]\n",
        "property_types = [\"apartment\", \"villa\", \"plot\", \"commercial space\"]\n",
        "locations = [\"Bangalore\", \"Mumbai\", \"Delhi\", \"Hyderabad\", \"Chennai\"]\n",
        "terms = [(\"carpet area\", \"built-up area\"), (\"freehold\", \"leasehold\"), (\"agreement\", \"sale deed\")]\n",
        "\n",
        "def generate_synthetic_answer(question):\n",
        "    q = question.lower()\n",
        "\n",
        "    if \"documents\" in q:\n",
        "        if \"selling\" in q:\n",
        "            return (\"You need the sale deed, property ownership papers, government-issued ID proof, \"\n",
        "                    \"tax receipts, and a no-objection certificate if applicable. Make sure all documents \"\n",
        "                    \"are original and verified.\")\n",
        "        elif \"buying\" in q or \"transferring\" in q:\n",
        "            return (\"Essential documents include property registration papers, identity proof (Aadhar, PAN), \"\n",
        "                    \"sale deed, encumbrance certificate, and latest tax receipts.\")\n",
        "        elif \"registering\" in q:\n",
        "            return (\"You will need the title deed, latest property tax receipts, a government-issued ID, \"\n",
        "                    \"and proof of possession for registration.\")\n",
        "        else:\n",
        "            return (\"Relevant documents typically include sale deed, ownership papers, tax receipts, \"\n",
        "                    \"and identity proof such as Aadhar or passport.\")\n",
        "\n",
        "    elif \"verify the authenticity\" in q:\n",
        "        entity = None\n",
        "        for e in entities:\n",
        "            if e in q:\n",
        "                entity = e\n",
        "                break\n",
        "        if entity == \"builder\":\n",
        "            return (\"Check the builder’s RERA registration number, completed project details, \"\n",
        "                    \"customer reviews, and official approvals from local authorities.\")\n",
        "        elif entity == \"title deed\":\n",
        "            return (\"Cross-check the title deed with local land records office or registrar's database \"\n",
        "                    \"to confirm ownership and absence of liens.\")\n",
        "        elif entity == \"property\":\n",
        "            return (\"Verify ownership through land registry records, confirm no pending dues, and \"\n",
        "                    \"consult a legal expert if needed.\")\n",
        "        elif entity == \"property tax receipt\":\n",
        "            return (\"Ensure the receipt matches the official municipal records, and check for the latest payment date.\")\n",
        "        else:\n",
        "            return (\"Verify with government records, cross-check all details, and seek legal advice if uncertain.\")\n",
        "\n",
        "    elif \"average price\" in q:\n",
        "        city_prices = {\n",
        "            \"bangalore\": \"₹70 lakhs to ₹1.2 crore, varying by locality and amenities\",\n",
        "            \"mumbai\": \"₹1.2 crore to ₹2.5 crore depending on the neighborhood and building quality\",\n",
        "            \"delhi\": \"₹80 lakhs to ₹1.5 crore depending on area and furnishing\",\n",
        "            \"hyderabad\": \"₹60 lakhs to ₹1 crore based on locality and builder reputation\",\n",
        "            \"chennai\": \"₹65 lakhs to ₹1.1 crore, influenced by proximity to city center and schools\"\n",
        "        }\n",
        "        for city in city_prices:\n",
        "            if city in q:\n",
        "                return (f\"The average price ranges between {city_prices[city]}. \"\n",
        "                        \"Prices can fluctuate based on amenities, builder, and market conditions.\")\n",
        "        return (\"Average prices depend on city, locality, property type, and current market trends.\")\n",
        "\n",
        "    elif \"legal considerations\" in q:\n",
        "        return (\"Ensure clear and marketable title, check encumbrance certificate for liens, verify property tax status, \"\n",
        "                \"and confirm there are no pending legal disputes before purchasing property.\")\n",
        "\n",
        "    elif \"difference between\" in q:\n",
        "        if \"freehold\" in q and \"leasehold\" in q:\n",
        "            return (\"Freehold ownership means you own the property and the land indefinitely, \"\n",
        "                    \"while leasehold means you have rights to use the property for a fixed period, \"\n",
        "                    \"after which ownership reverts to the landlord.\")\n",
        "        elif \"carpet area\" in q and \"built-up area\" in q:\n",
        "            return (\"Carpet area refers to the actual usable floor area within the walls of the property, \"\n",
        "                    \"whereas built-up area includes the carpet area plus the thickness of walls, balconies, \"\n",
        "                    \"and common areas proportionately.\")\n",
        "        elif \"agreement\" in q and \"sale deed\" in q:\n",
        "            return (\"An agreement to sell is a preliminary contract where the seller agrees to sell the property, \"\n",
        "                    \"while the sale deed is the final legal document that transfers ownership from seller to buyer.\")\n",
        "        else:\n",
        "            return (\"These terms have distinct legal definitions and implications; please refer to a legal expert for details.\")\n",
        "\n",
        "    return (\"This is a general real estate related answer, covering common queries and advice \"\n",
        "            \"for buyers and sellers.\")\n",
        "\n",
        "def generate_qa_pairs(n=200):\n",
        "    data = []\n",
        "    for i in range(n):\n",
        "        template = random.choice(questions_templates)\n",
        "        question = template.format(\n",
        "            scenario=random.choice(scenarios),\n",
        "            entity=random.choice(entities),\n",
        "            property_type=random.choice(property_types),\n",
        "            location=random.choice(locations),\n",
        "            term1=random.choice(terms)[0],\n",
        "            term2=random.choice(terms)[1]\n",
        "        )\n",
        "        answer = generate_synthetic_answer(question)\n",
        "        data.append({\"id\": str(i).zfill(4), \"question\": question, \"answer\": answer})\n",
        "    return data\n",
        "\n",
        "\n",
        "qa_data = generate_qa_pairs(200)\n",
        "df = pd.DataFrame(qa_data)\n",
        "json_path = \"real_estate_qa_improved.json\"\n",
        "df.to_json(json_path, orient=\"records\", indent=2)\n",
        "\n",
        "print(f\" Synthetic dataset saved to {json_path}\")\n"
      ],
      "metadata": {
        "id": "gVio9-fSncGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "login()\n"
      ],
      "metadata": {
        "id": "9jWDxi4FvbXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the synthetic dataset\n",
        "df = pd.read_json(\"real_estate_qa_improved.json\")\n",
        "\n",
        "# Create Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Push to HF Hub\n",
        "dataset.push_to_hub(\"foreseeitwithme/real-estate-qa-synthetic\", private=False)"
      ],
      "metadata": {
        "id": "_6riNTX-vvpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eee305c4"
      },
      "source": [
        "## 4. Model Fine-tuning\n",
        "\n",
        "Now that we have our synthetic dataset, let's fine-tune a small LLM using PEFT/LoRA techniques. This approach allows us to efficiently adapt the pre-trained model to our specific task without excessive computational requirements.\n",
        "\n",
        "We'll:\n",
        "1. Load the pre-trained model\n",
        "2. Prepare the dataset in the correct format\n",
        "3. Configure LoRA adapters\n",
        "4. Fine-tune the model\n",
        "5. Save the fine-tuned model\n",
        "\n",
        "This section uses Parameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA) to update only a small number of parameters, making it suitable for running on Colab's T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f5ad904"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"foreseeitwithme/real-estate-qa-synthetic\")\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)"
      ],
      "metadata": {
        "id": "U8egx-Uvwqp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen1.5-0.5B\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess(example):\n",
        "    prompt = f\"Question: {example['question']}\\nAnswer:\"\n",
        "    input_ids = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
        "    label_ids = tokenizer(example[\"answer\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    input_ids[\"labels\"] = label_ids[\"input_ids\"]\n",
        "    return input_ids\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "# Print relevant modules for LoRA targeting\n",
        "def print_relevant_modules(model):\n",
        "    print(\"Relevant modules in base model:\\n\")\n",
        "    for name, module in model.named_modules():\n",
        "        if any(x in name.lower() for x in [\"attn\", \"mlp\", \"ffn\", \"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"proj\"]):\n",
        "            print(name)\n",
        "\n",
        "print_relevant_modules(base_model)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZpnOAz7pYN__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"     # MLP\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "YBtEggvow0gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Define LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]  # Adjust this based on print_relevant_modules output\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(base_model, peft_config)\n",
        "\n",
        "# Print trainable params\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "59IYqjfvxf9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "# Define data collator for causal LM\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j--O_KvEzy3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f1df861"
      },
      "source": [
        "## 5. Model Evaluation\n",
        "\n",
        "Now that we have fine-tuned our model, let's evaluate its performance by comparing it with the base model. We'll assess how well our synthetic data helped improve the model's abilities on our target task.\n",
        "\n",
        "We'll:\n",
        "1. Load both the base and fine-tuned models\n",
        "2. Define appropriate evaluation metrics\n",
        "3. Perform inference on test examples\n",
        "4. Compare and analyze the results\n",
        "5. Visualize performance differences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IhmjP9SS0X5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"squad\")"
      ],
      "metadata": {
        "id": "PDub_Wce0Y5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "H3rOanG10qdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Reload and split the dataset\n",
        "dataset = load_dataset(\"foreseeitwithme/real-estate-qa-synthetic\")[\"train\"]\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)"
      ],
      "metadata": {
        "id": "p0ofcXu40r1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "DyenzqGb02iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"qwen1.5b-realestate-lora\")\n",
        "tokenizer.save_pretrained(\"qwen1.5b-realestate-lora\")"
      ],
      "metadata": {
        "id": "Xq-Yij0F7Txl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"qwen1.5b-realestate-lora\", trust_remote_code=True)\n",
        "model = PeftModel.from_pretrained(base, \"qwen1.5b-realestate-lora\")"
      ],
      "metadata": {
        "id": "3EHL7oib7Xh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_dir = \"/content/qwen1.5b-realestate-lora\"\n",
        "repo_name = \"foreseeitwithme/real-estate-qa-synthetic\"\n",
        "\n",
        "# Load model and tokenizer from local directory\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "# Push model and tokenizer to Hugging Face Hub\n",
        "model.push_to_hub(repo_name)\n",
        "tokenizer.push_to_hub(repo_name)\n",
        "\n",
        "print(f\"Model and tokenizer pushed to https://huggingface.co/{repo_name}\")\n"
      ],
      "metadata": {
        "id": "uKbXSpNIq4dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(example):\n",
        "    prompt = f\"\"\"Below is an instruction that describes a task. Write a short response that appropriately completes the request using only real estate terms with examples.\n",
        "\n",
        "### Instruction:\n",
        "You are a helpful real estate assistant providing clear and accurate answers.\n",
        "Answer the following real estate question accurate numbers and helpfully.\n",
        "\n",
        "### Question:\n",
        "{example['question']}\n",
        "\n",
        "### Response:\n",
        "{example['answer']}\"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
        "    input_ids[\"labels\"] = input_ids[\"input_ids\"].copy()\n",
        "    return input_ids\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "7iB2yjXHaduq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(s):\n",
        "    return s.lower().strip()\n",
        "\n",
        "def extract_first_sentence(text):\n",
        "    return re.split(r'(?<=[.!?])\\s', text.strip())[0]\n",
        "\n",
        "def extract_answer_from_output(text, question):\n",
        "    norm_text = text.lower().strip()\n",
        "    norm_question = question.lower().strip()\n",
        "    if norm_text.startswith(norm_question):\n",
        "        answer = norm_text[len(norm_question):].strip()\n",
        "    elif \"answer:\" in norm_text:\n",
        "        answer = norm_text.split(\"answer:\")[-1].strip()\n",
        "    else:\n",
        "        answer = norm_text\n",
        "    return extract_first_sentence(answer)"
      ],
      "metadata": {
        "id": "0HFrM3Uaae0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(model, tokenizer, dataset, max_samples=50, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    inputs = dataset.select(range(min(max_samples, len(dataset))))\n",
        "    prompts = [\n",
        "        f\"Question: {ex['question']}\\nProvide a specific answer using only real estate keywords and single sentence.\\nAnswer:\"\n",
        "        for ex in inputs\n",
        "    ]\n",
        "    tokenized = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **tokenized,\n",
        "            max_new_tokens=100,\n",
        "            num_beams=5,\n",
        "            early_stopping=True,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    preds_clean = [normalize_text(extract_answer_from_output(d, prompts[i])) for i, d in enumerate(decoded)]\n",
        "    refs = [normalize_text(ex[\"answer\"]) for ex in inputs]\n",
        "    return preds_clean, refs, inputs"
      ],
      "metadata": {
        "id": "vsLSN5J6aoks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dAkg2FcMjZXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def answer_question(user_question):\n",
        "    input_prompt = f\"\"\"You are a real estate expert. Provide a short and clear and answer in 1-2 sentences.\n",
        "\n",
        "### Question:\n",
        "{user_question}\n",
        "\n",
        "### Answer:\"\"\"\n",
        "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=150)\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "gr.Interface(fn=answer_question, inputs=\"text\", outputs=\"text\", title=\"Real Estate Q&A\").launch()\n"
      ],
      "metadata": {
        "id": "uP8NGzlkjaUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5YjvGGJamU2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import re\n",
        "from bert_score import score\n",
        "\n",
        "\n",
        "preds_clean, refs, inputs = generate_predictions(model, tokenizer, dataset[\"test\"], max_samples=50)\n",
        "\n",
        "\n",
        "P, R, F1 = score(preds_clean, refs, lang=\"en\", verbose=True)\n",
        "\n",
        "\n",
        "avg_precision = P.mean().item()\n",
        "avg_recall = R.mean().item()\n",
        "avg_f1 = F1.mean().item()\n",
        "\n",
        "print(\"BERTScore:\")\n",
        "print(f\"Precision: {avg_precision:.4f}\")\n",
        "print(f\"Recall:    {avg_recall:.4f}\")\n",
        "print(f\"F1 Score:  {avg_f1:.4f}\")\n",
        "\n",
        "# Print some examples\n",
        "for i in range(5):\n",
        "    print(f\"\\nQ: {inputs[i]['question']}\")\n",
        "    print(f\"Predicted Answer: {preds_clean[i]}\")\n",
        "    print(f\"Reference Answer: {refs[i]}\")\n"
      ],
      "metadata": {
        "id": "bTXs-tk8lVgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "bert_results = {\n",
        "    \"Precision\": 0.8549,\n",
        "    \"Recall\": 0.8638,\n",
        "    \"F1 Score\": 0.8591\n",
        "}\n",
        "\n",
        "\n",
        "def plot_bert_scores(scores_dict):\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    metrics = list(scores_dict.keys())\n",
        "    values = list(scores_dict.values())\n",
        "\n",
        "    ax = sns.barplot(x=metrics, y=values, palette=\"viridis\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"BERTScore Evaluation\")\n",
        "    for i, v in enumerate(values):\n",
        "        ax.text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontweight='bold')\n",
        "\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.show()\n",
        "\n",
        "plot_bert_scores(bert_results)\n"
      ],
      "metadata": {
        "id": "tvnX6yjlujvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ef42d8"
      },
      "source": [
        "## 6. Final Thoughts and Project Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Report\n",
        "\n",
        "---\n",
        "\n",
        "#### **Project Summary**\n",
        "\n",
        "**Use Case:**\n",
        "I built a Q\\&A assistant focused on the **Indian real estate domain**, which frequently deals with queries about property documentation, legal terminology, and pricing. This use case was chosen because of its repetitive structure and the need for domain-specific accuracy — making it ideal for fine-tuning a language model.\n",
        "\n",
        "**Synthetic Dataset:**\n",
        "I first generated 200+ question-answer pairs using templated questions such as:\n",
        "\n",
        "* “What documents are required for {scenario}?”\n",
        "* “What is the difference between {term1} and {term2}?”\n",
        "* “What is the average price of a {property\\_type} in {location}?”\n",
        "\n",
        "These were filled with combinations of common Indian real estate scenarios and legal terms (like “sale deed” vs “agreement”, “freehold” vs “leasehold”). The answers were designed to simulate domain expertise, using structured, informative text.\n",
        "\n",
        "**Model Fine-Tuned:**\n",
        "I fine-tuned **Qwen/Qwen1.5-0.5B**, a 0.5B parameter model, using **QLoRA** (quantized LoRA), which enabled efficient training with minimal GPU memory. Training was performed for 3 epochs using the hugging face Transformers + PEFT ecosystem.\n",
        "\n",
        "**Evaluation Metric:**\n",
        "We used **BERTScore** to measure the semantic similarity between generated and reference answers. The fine-tuned model achieved:\n",
        "\n",
        "* **Precision:** 0.8549\n",
        "* **Recall:** 0.8638\n",
        "* **F1 Score:** 0.8591\n",
        "\n",
        "---\n",
        "\n",
        "#### **Analysis of Results**\n",
        "\n",
        "**Did Fine-Tuning Improve Performance?**\n",
        "Yes. The model became more consistent and domain-aware. Generic LLMs struggled with specific legal distinctions, while the fine-tuned model correctly differentiated concepts like “agreement vs sale deed” or “freehold vs leasehold”.\n",
        "\n",
        "**Where Was Improvement More Noticeable?**\n",
        "\n",
        "* **Legal differentiation** questions showed the most gain.\n",
        "* **Document requirements** became clearer and more structured.\n",
        "* **Average pricing** answers became more aligned with regional data.\n",
        "\n",
        "**Limitations Observed:**\n",
        "\n",
        "* Responses still lack nuance in very context-specific questions (e.g., based on sub-localities).\n",
        "* Some answers are overgeneralized due to the templated nature of the dataset.\n",
        "* Lack of reasoning or multi-step inference in a few queries.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Improvement Ideas**\n",
        "\n",
        "* Use more **diverse templates** and add real user queries from forums for better generalization.\n",
        "* Generate answers using **LLMs + domain expert review** to simulate more realistic variability.\n",
        "* Explore **instruction tuning** or **RAG (Retrieval-Augmented Generation)** approaches for more grounded answers.\n",
        "* With more compute, fine-tune a larger model (1.3B–3B) and increase dataset size to 5k+ QA pairs.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Learning Outcomes**\n",
        "\n",
        "* Learned how **synthetic data**, when thoughtfully designed, can significantly improve model performance in a narrow domain.\n",
        "* Understood the efficiency of **QLoRA + PEFT** for fine-tuning large models with limited hardware.\n",
        "* Surprised by how well a 0.5B model performed after fine-tuning — producing domain-specific, coherent answers.\n",
        "* Gained experience in evaluating models with **semantic similarity metrics** like BERTScore, which are more suitable than traditional token-based metrics for open-ended QA.\n",
        "\n"
      ],
      "metadata": {
        "id": "5T8vrrFSvYOz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "398ed131"
      },
      "source": [
        "## 7. References\n",
        "\n",
        "1. [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft/index)\n",
        "2. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "3. [Parameter-Efficient Fine-Tuning Methods](https://huggingface.co/blog/peft)\n",
        "4. [Synthetic Data Generation Techniques](https://arxiv.org/abs/2111.02739)\n",
        "5. [Evaluating Large Language Models](https://arxiv.org/abs/2307.03109)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### other than the above mentioned resources,the following video tutorials helped me complete the assignment\n",
        "1. [synthetic data generation](https://www.youtube.com/watch?v=iogrvDu5K0k&utm_source=chatgpt.com)\n",
        "2. [fine tuning LLM](https://www.youtube.com/watch?v=kWooqJKJO7k&utm_source=chatgpt.com)"
      ],
      "metadata": {
        "id": "oVaS_vHpxjCb"
      }
    }
  ]
}